{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "from PyPDF2 import PdfFileWriter, PdfFileReader\n",
    "from requests_html import HTMLSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the set of links (unique links)\n",
    "internal_urls = set()\n",
    "external_urls = set()\n",
    "pdf_urls = set()\n",
    "urls = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will make sure that a proper scheme (protocol, e.g http or https) and domain name exists in the URL.\n",
    "def is_valid(url):\n",
    "    \"\"\"Checks whether `url` is a valid URL\"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    return bool(parsed.netloc) and bool(parsed.scheme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_website_links(url):\n",
    "    session = HTMLSession()\n",
    "    response = session.get(url)\n",
    "    try:\n",
    "        response.html.render()\n",
    "    except:\n",
    "        pass\n",
    "#     soup = BeautifulSoup(requests.get(url).content, \"html.parser\")\n",
    "    domain_name = urlparse(url).netloc\n",
    "    soup = BeautifulSoup(response.html.html, \"html.parser\")  # this is assumed for javascript loaded pages \n",
    "    for a_tag in soup.findAll(\"a\"):\n",
    "        href = a_tag.attrs.get(\"href\")\n",
    "#         print(href)\n",
    "        if href == \"\" or href is None:\n",
    "            # href empty tag\n",
    "            continue\n",
    "        if href.endswith(\".pdf\"):\n",
    "            pdf_urls.add(href)\n",
    "        \n",
    "        href = urljoin(url, href)\n",
    "        parsed_href = urlparse(href)\n",
    "        if parsed_href.query != \"\":\n",
    "            href = parsed_href.scheme + \"://\" + parsed_href.netloc + parsed_href.path + \"?\" + parsed_href.query\n",
    "        else:\n",
    "            href = parsed_href.scheme + \"://\" + parsed_href.netloc + parsed_href.path\n",
    "        \n",
    "        \n",
    "        if not is_valid(href):\n",
    "            # not a valid URL\n",
    "            continue\n",
    "        if href in internal_urls:\n",
    "            # already in the set\n",
    "            continue\n",
    "        if domain_name not in href:\n",
    "            # its external link\n",
    "            if href not in external_urls:\n",
    "                external_urls.add(href)\n",
    "            continue\n",
    "        # else its internel links\n",
    "        urls.add(href)\n",
    "        internal_urls.add(href)\n",
    "    return urls\n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_urls_visited = 0\n",
    "\n",
    "# help to call give number of times inside the website\n",
    "def crawl(url, max_urls=10):\n",
    "    \"\"\"\n",
    "    Crawls a web page and extracts all links.\n",
    "    You'll find all links in `external_urls` and `internal_urls` global set variables.\n",
    "    params:\n",
    "        max_urls (int): number of max urls to crawl, default is 10.\n",
    "    \"\"\"\n",
    "    global total_urls_visited\n",
    "    total_urls_visited += 1\n",
    "    links = get_all_website_links(url)\n",
    "    for link in links:\n",
    "        print(link)\n",
    "        if total_urls_visited > max_urls:\n",
    "            break\n",
    "        crawl(link, max_urls=max_urls)\n",
    "        print(total_urls_visited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.ijser.org/ResearchPaperPublishing_February2011.aspx\n",
      "https://www.ijser.org/ResearchPaperPublishing_February2011.aspx\n",
      "https://www.ijser.org/ResearchPaperPublishing_February2011.aspx\n",
      "3\n",
      "https://www.ijser.org/ResearchPaperPublishing_March2011.aspx\n",
      "3\n",
      "https://www.ijser.org/ResearchPaperPublishing_March2011.aspx\n",
      "[+] Total External links: 6\n",
      "[+] Total Internal links: 191\n",
      "[+] Total: 197\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "root_url  =  \"https://ijser.org/\"\n",
    "if __name__ == \"__main__\":\n",
    "   \n",
    "    crawl(root_url,max_urls = 2)\n",
    "    \n",
    "    \n",
    "    print(\"[+] Total External links:\", len(external_urls))\n",
    "    print(\"[+] Total Internal links:\", len(internal_urls))\n",
    "    print(\"[+] Total:\", len(external_urls) + len(internal_urls))\n",
    "    \n",
    "#     domain_name = urlparse(root_url).netloc\n",
    "    \n",
    "#     # save the internal links to a file\n",
    "#     with open(f\"{domain_name}_internal_links.txt\", \"w\") as f:\n",
    "#         for internal_link in internal_urls:\n",
    "#             print(internal_link.strip(), file=f)\n",
    "\n",
    "#     # save the external links to a file\n",
    "#     with open(f\"{domain_name}_external_links.txt\", \"w\") as f:\n",
    "#         for external_link in external_urls:\n",
    "#             print(external_link.strip(), file=f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'www.geeksforgeeks.org'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed = urlparse(\"https://www.geeksforgeeks.org/k-nearest-neighbours/\")\n",
    "netloc = parsed.netloc\n",
    "netloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory  pdfs\\org  Created \n",
      "Directory  txts\\org  Created \n"
     ]
    }
   ],
   "source": [
    "#Create pdfs and txts \n",
    "import os\n",
    "parsed = urlparse(root_url)\n",
    "netloc = parsed.netloc\n",
    "\n",
    "if not os.path.exists(\"pdfs\"):\n",
    "    os.mkdir(\"pdfs\")\n",
    "\n",
    "if not os.path.exists(\"txts\"):\n",
    "    os.mkdir(\"txts\")\n",
    "    \n",
    "pdf_dirName = os.path.join(\"pdfs\",netloc.split(\".\")[1])\n",
    "txt_dirName = os.path.join(\"txts\",netloc.split(\".\")[1])\n",
    "\n",
    "def make_dir(dirName):\n",
    "    if not os.path.exists(dirName):\n",
    "        os.mkdir(dirName)\n",
    "        print(\"Directory \" , dirName ,  \" Created \")\n",
    "    else:\n",
    "        print(\"Directory \" , dirName ,  \" already exists\")\n",
    "\n",
    "make_dir(pdf_dirName)\n",
    "make_dir(txt_dirName)\n",
    "make_dir(domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'txts\\\\org'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_dirName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pdfs 16!\n"
     ]
    }
   ],
   "source": [
    "internal_urls = list(internal_urls)\n",
    "pdf_urls = [iu for iu in internal_urls if iu.endswith(\".pdf\")]\n",
    "\n",
    "total_pdfs = len(pdf_urls)\n",
    "print(f\"Total pdfs {total_pdfs}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://www.ijset.net/journal/741.pdf'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_urls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dowload_pdf_save_txt(url):\n",
    "    print(url)\n",
    "    pdf_name = url.split(\"/\")[-1]\n",
    "    print(pdf_name)\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        with open(f'{pdf_dirName}/{pdf_name}', 'wb') as f:\n",
    "            f.write(response.content)\n",
    "            f.close()\n",
    "        output = PdfFileWriter()\n",
    "        inputs = PdfFileReader(open(f'{pdf_dirName}/{pdf_name}', \"rb\"))\n",
    "        print (f\"{pdf_name} has {inputs.getNumPages()} pages.\")\n",
    "        total_pages = inputs.getNumPages()\n",
    "        txt = \"\"\n",
    "        for i in range(total_pages):\n",
    "            txt+=inputs.getPage(i).extractText()\n",
    "\n",
    "        with open(f'{txt_dirName}/{pdf_name.split(\".\")[0]}.txt', 'w+') as f:\n",
    "            f.write(txt)\n",
    "            f.close()\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.ijset.net/journal/741.pdf\n",
      "741.pdf\n",
      "741.pdf has 9 pages.\n",
      "http://www.ijset.net/journal/2323.pdf\n",
      "2323.pdf\n",
      "2323.pdf has 8 pages.\n",
      "Total 2 pdfs downloaded:\n",
      "!10.378966093063354 \" seconds take!\n",
      "Lets see pdfs and txts folder\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t1 = time.time()\n",
    "count = 0\n",
    "select_number_of_pdf = 2\n",
    "for pdf_url in pdf_urls[:select_number_of_pdf]:\n",
    "    count+=1\n",
    "    dowload_pdf_save_txt(pdf_url)\n",
    "print(f\"Total {count} pdfs downloaded:\")\n",
    "t2 = time.time()\n",
    "print(f'!{t2-t1} \" seconds take!')\n",
    "print(\"Lets see pdfs and txts folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document1.pdf has 7 pages.\n"
     ]
    }
   ],
   "source": [
    "#Convert pdfs to txts\n",
    "output = PdfFileWriter()\n",
    "input1 = PdfFileReader(open(\"ijser.pdf\", \"rb\"))\n",
    "# print how many pages input1 has:\n",
    "print (\"document1.pdf has %d pages.\" % input1.getNumPages())\n",
    "total_pages = input1.getNumPages()\n",
    "\n",
    "txt = \"\"\n",
    "for i in range(total_pages):\n",
    "    txt+=input1.getPage(i).extractText()\n",
    "\n",
    "with open(f'{txt_dirName}/one.txt', 'w+') as f:\n",
    "    f.write(txt)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser(description=\"Link Extractor Tool with Python\")\n",
    "parser.add_argument(\"url\", help=\"The URL to extract links from.\")\n",
    "parser.add_argument(\"-m\", \"--max-urls\", help=\"Number of max URLs to crawl, default is 5.\", default=5, type=int)\n",
    "\n",
    "args = parser.parse_args()\n",
    "root_url = args.url\n",
    "max_urls = args.max_urls\n",
    "\n",
    "\n",
    "\n",
    "domain_name = urlparse(url).netloc\n",
    "\n",
    "# save the internal links to a file\n",
    "with open(f\"{domain_name}_internal_links.txt\", \"w\") as f:\n",
    "    for internal_link in internal_urls:\n",
    "        print(internal_link.strip(), file=f)\n",
    "\n",
    "# save the external links to a file\n",
    "with open(f\"{domain_name}_external_links.txt\", \"w\") as f:\n",
    "    for external_link in external_urls:\n",
    "        print(external_link.strip(), file=f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "'Python Interactive'",
   "language": "python",
   "name": "82608ab3-5579-42f8-bcbb-22d2ccb1eaff"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
